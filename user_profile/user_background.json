{
  "current_title": "Senior Software Engineer",
  "years_experience": 8,
  "education": "M.S. in Computer and Information Technology - University of Pennsylvania (Jan 2020 - Dec 2021): Built foundation of software engineering, including data structure, algorithm, computer system, operating system, software design, database, machine learning and computer vision. Earned Master's degree while holding full-time job, GPA: 3.97. Ph.D. in Materials Science and Engineering - Virginia Tech (Sep 2012 - May 2015): Research focused on nanomaterial-based semiconductor material for power semiconductor chips. Published 11 peer-reviewed papers, GPA: 3.50.",
  "languages": [
    "Python",
    "JavaScript",
    "TypeScript",
    "Java",
    "Golang",
    "Scala",
    "Rust",
    "SQL",
    "HTML/CSS"
  ],
  "technologies": [
    "Spring",
    "Django",
    "Node.js",
    "React",
    "Next.js",
    "PostgreSQL",
    "DynamoDB",
    "Cassandra",
    "Redis",
    "Kafka",
    "Elasticsearch",
    "Spark",
    "Airflow",
    "Flink"
  ],
  "infrastructure": [
    "Docker",
    "Kubernetes",
    "Jenkins",
    "GitHub Actions",
    "Terraform",
    "Prometheus",
    "Grafana",
    "AWS CloudFormation",
    "AWS CDK"
  ],
  "professional_experience": [
    {
      "company_name": "Robinhood Markets, Inc.",
      "location": "Remote",
      "job_title": "Senior Software Engineer - Backend",
      "time": "Mar 2025 - Present",
      "projects": [
        {
          "title": "Lead the Futures/Event contract backend service internationalization at Robinhood",
          "context": "As Robinhood expanded its Futures trading capabilities to international markets, a critical requirement emerged — the backend services handling Futures and Event contracts were originally built with U.S.-specific assumptions around contract types, market hours, timezones, and regulatory compliance. To support global trading, the platform needed to be refactored and extended for internationalization (i18n), ensuring it could dynamically serve users across multiple regions with varying exchange configurations and asset definitions. This project was prioritized as foundational infrastructure work for Robinhood's global trading roadmap.",
          "action": "I led the effort by first designing the technical architecture for the internationalization framework and authored a comprehensive design document, which was reviewed and approved by the team's tech lead. The design introduced region-aware abstractions and a flexible data access pattern to support localized behavior without disrupting existing functionality. I implemented both service-layer and database-level regional sharding to isolate data paths by geography, improving scalability and data integrity. This involved updating core service logic and refactoring all relevant Kafka producer and consumer pipelines to handle region-specific routing, schema validation, and fault-tolerant delivery. Throughout the process, I collaborated cross-functionally with product, compliance, and SRE teams to align on requirements, rollout plans, and operational safety.",
          "achievement": "The internationalized backend framework was successfully launched, enabling Robinhood to support region-specific Futures and Event contracts for the first time. This unlocked the path to global expansion in derivatives trading and established the foundation for future cross-market features. The modular, sharded architecture has already been reused for new regional launches, significantly accelerating onboarding for additional markets. This project demonstrates my ability to drive technical design, lead cross-functional execution, and modernize infrastructure for long-term scalability."
        },
        {
          "title": "Addressed legal compliance issues across users' Futures account lifecycle at Robinhood",
          "context": "The Legal and Operations teams raised critical compliance concerns regarding the lifecycle of users' Futures accounts. Our team owned both the account management service and the underlying account data, placing us in a central role to resolve these risks. Key issues included ensuring users with specific tax-related statuses were appropriately restricted from trading, and that any suspension of a user's brokerage account would cascade to their Futures account as required by regulatory standards. Addressing these concerns was urgent to maintain legal compliance and avoid potential issues during external audits.",
          "action": "I took ownership of the project and began by working closely with Legal and Ops stakeholders to translate regulatory requirements into concrete technical rules. I designed and implemented a solution that combined incremental batch jobs with Kafka stream consumers, enabling our systems to update Futures account statuses in near real-time based on external triggers (e.g., brokerage account suspensions, tax status changes). Additionally, I developed a daily reconciliation job to detect and correct any inconsistencies between the authoritative brokerage account state and the Futures system. These changes were implemented with careful attention to auditability and rollback safety, including detailed logging and operational dashboards for monitoring.",
          "achievement": "The system enhancements I delivered fully addressed the legal and operational compliance concerns, reducing regulatory risk and ensuring the Futures platform was audit-ready. The new processes now guarantee that only eligible users can access Futures trading, and that suspensions are enforced promptly and accurately. This project exemplifies my ability to translate ambiguous compliance requirements into robust, scalable systems, and to proactively protect the business from regulatory exposure."
        }
      ]
    },
    {
      "company_name": "Amazon.com, Inc.",
      "location": "Irvine, CA",
      "job_title": "Software Development Engineer II",
      "time": "Jul 2023 - Feb 2025",
      "projects": [
        {
          "title": "Lead and launch Amazon's flagship social proof feature - 'X+ bought in past month'",
          "context": "Product team did a market analysis from competitors, and believe that Amazon is lack of a social proof feature such as monthly sale counts. This is a extremely helpful information for the customer when shopping and comparing products from different sellers. Upper management from the entire shopping org approved the project of creating this social proof feature. This project has a very large scope and huge impact, it requires significant project leading skills and cross-team collaboration.",
          "action": "After promotion, I showed my strong eager to take on projects that has large scope, and thus get assigned to lead this project of creating the social proof feature. As the project lead, I was responsible for designing and implementing APIs and core business logic that supported this feature across critical surfaces like Search, Detail pages, and Cart. I collaborated closely with frontend engineers, data engineers, and DevOps teams to ensure seamless integration. One of the most difficult aspects was building the backend in Amazon's in-house language based on Ion, which required deep learning and adaptation to a specialized toolset.The complexity of the project was amplified by the need to scale efficiently for billions of customers while ensuring data accuracy and compliance. To address these challenges, I implemented robust data validation pipelines and caching strategies, reducing latency and ensuring a high-quality user experience. I also conducted rigorous A/B testing to validate the feature's impact, iterating on the implementation based on customer engagement metrics.",
          "achievement": "Through careful planning and execution, I successfully delivered this social proof feature and launched it ('X+ bought in past month') on Search, Detail pages, and Cart, which not only boosted customer trust and engagement but also drove a significant $400 million increase in annual revenue. This project exemplifies my ability to lead cross-functional teams, solve technical challenges, and deliver high-impact solutions at scale. It remains one of the highlights of my career, showcasing my passion for building customer-centric, data-driven products."
        },
        {
          "title": "Tier-1 service NAWS (native AWS) migration",
          "context": "Our org has a Tier-1 service called DigitalRecommendationsService. It is a critical service that generates recommendation strategies for various surfaces such as retail websites, fire tablet, kindle stores, etc. The average TPS for the service is over 1 million per second. The main issue for this service is that is over 10 years old and it is based on some very outdated technologies such as VM-based hosts, hardware-based load balancer, manual scaling for peak events, etc. The operational burden of this service is very high, and the oncall engineers have to wait up in the mid of the night to fix issues on this service. To address this issue, our org established a core team with several senior SDEs, the goal is to migrate this Tier-1 service into using more advanced technologies by using native AWS.",
          "action": "I'm always interested in infra related projects, and I noticed the huge impact of this migration project that aligns perfectly with my interest. So I volunteered to join the core team and worked on several key tasks on this project. One task involved to migrate part of the service logics to NAWS by establish a new service using AWS ECS. I created the infra using AWS CDK, added CI/CD pipelines and monitoring to make sure the new service meets the Tier-1 requirements, such as five-night reliability, P90 latency under 200ms, etc. The other task is to migrate the load balancer from old hardware LB to AWS ALB. During the migration, unexpected network routing issues arose, threatening to delay the project. I took the initiative to troubleshoot the configuration errors, leveraging both documentation and consultation with cross-team network specialists. I carefully validated each change in a test environment to minimize risks before implementing it in production. To ensure no service disruptions, I established a detailed rollback plan and monitored the migration closely during the cutover.",
          "achievement": "I successfully migrated over 50% of this Tier-1 services into the more advanced native AWS infra, which reduced infra cost by over 30%. During the migration, I also fixed multiple legacy issues in the code base. Besides the new service is also autoscaling. These efforts helped to reduce the operation cost and oncall burden by 25% reduction of oncall tickets."
        },
        {
          "title": "Addressing Escalation from CEO for Haul store order confirmation email",
          "context": "With the launch of the Haul store, which sells items at ultra-low prices under $20, our CEO Andy Jessy tested the store by making a purchase. When he received the order confirmation email, there is a recommendation widget in the email that is still recommending products from the regular retail store which were well over $20. This shopping experience is not ideal for customer who is shopping on Haul, and he submitted an escalation to fix this widget to only show Haul products within a week.",
          "action": "This is an extremely urgent task, and our team is the owner of this recommendation widget. After receiving this escalation from CEO, I immediately volunteered to take on this task. Right at the start, I created a robust tech design which broke down the task into multiple required steps. Then I created a task force team that includes engineers from Haul store, email delivery service team, and their corresponding managers. I assigned owners for each step, and I hosted twice daily standup meetings to update progress on the tasks, to make sure we resolve this issue before the 1-week deadline.",
          "achievement": "We implemented multiple components that includes email templating logic change, Haul-based banner and recommendation strategies, and frontend component for Haul content. And we launched these features successfully before the deadline. We received recognition from CEO for demonstrating Bias for Action and delivering high-quality results at speed."
        },
        {
          "title": "Digital Markets Act (DMA) compliance ownership",
          "context": "The Digital Markets Act (DMA) is a regulation introduced by the European Union to ensure fair competition in the digital market. Our org's software is considered in-scope for DMA compliance, and there are multiple service and datasets that owned by the entire org that need to be examined and making sure to update them according to the compliance requirements.",
          "action": "I led the effort of creating and implementing the compliance plan for the entire org, which includes smaller sister teams. To start, I conducted a comprehensive review of a list of documents provided by the Privacy team. This thorough analysis enabled me to distill the most relevant information for our team, which I then organized into a detailed summary document. I presented this summary document to our team members, ensuring everyone was informed and aligned with the necessary steps. Additionally, I proactively engaged with PoCs from the sisters teams in the org by hosting weekly meetings, to ensure a collaborative approach and progress tracking that all the teams execute and implement the compliance plan within the target deadline.",
          "achievement": "Our org becomes the first org that completed DMA compliance implementation through the entire Personalization devision at Amazon. We received praises from up managements for bias for action, and deliver results in a timely fashion."
        },
        {
          "title": "Developed a Linear Bayesian ML model for Amazon's Digital contents (Ebook, Audible, Prime Videos, Music)",
          "context": "Amazon's Digital team was exploring further promoting digital contents on Amazon's retail stores. For that purpose, they wanted to leveraging our team's capability to generate better recommendation strategies. We have a powerful machine learning system that can do blending and re-ranking on products, which is a great use case for Digital team.",
          "action": "I explored the feasibility of integrating this ML system with digital contents. Initially, my design involved directly utilizing the Unified models from the our team already own, to test on digital content. However, after receiving feedback from a senior SDE, I recognized the drawbacks of my initial design and revised the plan to create a new model specifically tailored for digital content. Although this new design is more time-consuming and technically complex, it is undoubtedly beneficial in the long run because the digital-specific model will be more effective for digital content. During the implementation, I overcame several challenges. Firstly, there was no training data available for digital contents. I collaborated with SDEs from Digital Team to get the required training data. Additionally, when testing the model, I encountered a cache configuration issue within one of our ML service, which I managed to resolve this issue independently by consulting the documentation.",
          "achievement": "A new model specifically for digital contents was created and trained successfully. I used this new model to generate a new recommendation strategy called 'Top picks for you in digital', which was launched worldwide after A/B testing. The test results indicating a 30 million revenue growth for digital contents."
        }
      ]
    },
    {
      "company_name": "Amazon.com, Inc.",
      "location": "Irvine, CA",
      "job_title": "Software Development Engineer I",
      "time": "Feb 2022 - Jun 2023",
      "projects": [
        {
          "title": "Data ETL pipelines migration to native AWS",
          "context": "My teams owns several old data ETL pipelines that are running on VM-based old hosts. These pipelines cannot be deprecated due to their roles of supporting some key recommendation features. But these old infra are very costly, and these ETL pipelines are mostly daily batch jobs and the hosts are running idol most of the time. There is a lot of infra savings can be made by migrating these pipelines to native AWS.",
          "action": "I took on this project as a new hire SDE. I quickly grasped the required improvements, and explored all the possible tools that can be used, such as AWS S3, Glue, Lambda, Step Functions, Batch, ECS and Fargate. I created a design doc by listing all the possible migration solutions for each ETL pipelines, which was reviewed and approved by TL and Senior SDEs. Then I created new infra for all these ETL pipelines using native AWS techniques, and manage them through AWS CDK.",
          "achievement": "I finished migration for total 5 ETL pipelines from old VM-based infra to native AWS. The migration resulted in a 90% reduction in infrastructure costs."
        },
        {
          "title": "Internal tooling (Cradle, an Glue-based batch job scheduler) infra and codebase migration",
          "context": "Our organization has only has one large package for Cradle infrastructure that creates unnecessary blocks for different teams and infra cost unclearness. Teams under the org need to migrate Cradle resources (infra and codebase) to their own infrastructures.",
          "action": "I lead this project for my team. Under the guidance of an Senior SDE, I created all the necessary infra: CDK pipelines and all the necessary packages. Then I presented the new infra to my team and provided clear instructions for codebase and resource migration tasks for all 10 team members.",
          "achievement": "Through my leadership and project management, our team successfully migrated all 26 Cradle profiles within a week. Now our team's Cradle builds will no longer interfere with other teams' builds, that provided significant improvement on the operation control, reduced the CI/CD pipeline blockage time by 95%. Our team also had much clear view on the infra cost, which helped the team to pinpoint the inefficiency of the builds and consequently reduced the infra cost by 30%."
        },
        {
          "title": "1P device ML model testing and launch",
          "context": "Data science team of 1P device org developed a new Neural-Network based ML model to predict the 1P devices that the customer is most likely to purchase in the next 15 days. The team wants to test the performance of the model, but they lack the domain knowledge on recommendation system our company owns.",
          "action": "As the owner of Amazon's powerful recommendation system. I lead this project by collaborating with their team. I hosted multiple meetings with their team to clarify there requirements. Then I design the testing plan for them, such as how to create the recommendation strategy based on their model, which page and which slot to use, and how to do A/B testing to validate the performance of their model.",
          "achievement": "The create data pipeline for serving their ML model, and successfully launched a new recommendation strategy for 1P device team. A/B testing results indicate that this new strategy generates over 20 million yearly revenue growth and 1 million extra 1P device sales."
        },
        {
          "title": "Audible à la carte (ALC) offer testing and launch",
          "context": "Audible team has a new pricing model ALC which shows the single-item purchase price. After market analysis, Audible team believed this pricing model could provide better shopping experience to Audible customers.",
          "action": "This project starts with ambiguities on how and where to test the ALC pricing. With extensive research, I found all Audible-related pages and recommendation slots that our team owns, and then compared different options and trade-offs between them. Then I designed the experiment to test ALC pricing by utilizing our frontend library. Because of the large impact of the library, I went through very strict code reviews and successfully dialed up the A/B testing. I also did an deep dive on the experiment results, that includes resolving Audible metric issues, providing additional results such as Audible listening days, signups, downstream revenues, and bullseye segmentation analysis to compare members and non-members.",
          "achievement": "With all the positive impacts I shown, Audible team eventually made the decision to launch the ALC pricing model worldwide, that lead to a 5 million annual Audible sales revenue."
        },
        {
          "title": "Audible Premium Plus (PP) member CX improvement",
          "context": "After we launched the ALC pricing worldwide for Audible, we believe there are opportunities to further improve the shopping experience, especially for PP members as they can purchase books with their monthly credits.",
          "action": "With the requirement in mind, I designed a new CX by combining ALC price with the credit purchase option. To ensure the CX with the highest quality for our customers, I collaborated with various UX design teams within Audible and P13N. This collaboration focused on selecting appropriate font types, colors, sizes, and ensuring accurate translations. After finalizing the new CX, I implemented it in our frontend library. I captured screenshots of the new CX across all the slots, which were reviewed and approved by the all the stakeholders.",
          "achievement": "A/B testing results on this new CX shows that this initiative resulted in a 2 million annual Audible sales revenue. We launched this new CS worldwide where Audible is launched."
        }
      ]
    },
    {
      "company_name": "TRUMPF Photonics, Inc.",
      "location": "Cranbury, NJ",
      "job_title": "Senior R&D Engineer",
      "time": "Jan 2019 - Dec 2021",
      "projects": [
        {
          "title": "Automation recipe development for laser chip bonding equipment",
          "context": "TRUMPF has a semiconductor packaging manufacture production line, which has many laser chip bonding machines. Before I join the company, all these equipments are operated by technicians. There has been consistent human turnovers and operation errors that causes production yield loss, and machine downtime.",
          "action": "With my previous knowledge and experience on Computer-vision and machine learning based automation, I lead the project of establish the automation workflows for these laser chip bonding machines. I quickly onboarded to the machines and tools, and immediately pointed out the pain points and key features that need to be implemented. Then I spend several months working and successfully created several automation recipes using python and Convolutional Neural Network (CNN) models.",
          "achievement": "Developed pattern recognition automation recipes with CNN models for high-precision laser chip bonding equipment, achieving sub-micron key-alignment precision. The recipes significantly reduced human errors by 95%, and improved production yield by 20%."
        },
        {
          "title": "Create BI reporting system for C-Suite management",
          "context": "The site CEO needs to create reports and presentations for monthly all hands meeting, as well as reporting back to the headquarter. Almost all the reporting data from production are collected through excel sheets and manual charting.",
          "action": "With my knowledge on python, I worked with IT department and created a BI reporting system. I created several data pipelines using Apache Airflow and Spark. I built multiple batch jobs to collect and aggregate data from production line, and created charts and dashboards using Kibana.",
          "achievement": "The BI reporting system I created reduced over 90% of the manual reporting work. Most of the data collection and processing are automated, which made sure that data integrity is secured, and there was no error in those key reports."
        }
      ]
    },
    {
      "company_name": "IBM / GlobalFoundries",
      "location": "Malta, NY",
      "job_title": "Senior R&D Engineer",
      "time": "May 2015 - Nov 2018",
      "projects": [
        {
          "title": "Automation recipe development for Transmission Electron Microscopy (TEM)",
          "context": "As a billion-dollar semiconductor chip manufacturer (wafer fab), GlobalFoundries utilize an equipment named Transmission Electron Microscopy to do failure analysis and material characterization. This equipment can take digital images on well-manufactured samples, but it is very complicated equipment and requires significant amount of training to operate on it, and a lot of practice to get the great image qualities. Because of the huge demand of the images from the fab, the team hired many technicians to run 24 X 7. Lots of turnovers and human errors caused bad images, reworks, as well as equipment damages. They need a solution to improve the productivity and effectiveness of the equipment. The solution here is create an automation on the imaging process.",
          "action": "I volunteered to take on this challenge despite I have no prior knowledge on coding, computer vision and automation. I spent several months (mostly my own spare time off work) to study computer vision, python programming, machine learning and deep learning. Then I created over 50 automation recipes for the equipment by utilizing python modules like OpenCV, scikit-learn, and TensorFlow.",
          "achievement": "I established the complete automation flow for the equipment, which doubled the productivity with improved image qualities. (correct and high quality images generated). With that automation flow, I also reduced the equipment downtime due to human errors by 30%."
        }
      ]
    }
  ],
  "target_roles": [
    "Software Development Engineer",
    "Senior Software Engineer",
    "Software Developer",
    "Full Stack Engineer",
    "Backend Engineer",
    "AI Engineer",
    "Data Engineer"
  ],
  "match_goal": "Seeking a mid-level or senior software engineer position where I can leverage my skills and experience to drive high-impact projects while continuing to grow professionally. If job requirements align very well with my skills, entry-level positions are also acceptable.",
  "location_preferences": ["Remote"],
  "salary_range": "$100,000 - $250,000",
  "work_preferences": ["Remote"]
}
